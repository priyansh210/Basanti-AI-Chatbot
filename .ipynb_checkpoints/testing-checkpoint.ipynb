{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "f21b7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import dateparser\n",
    "import pygwalker as pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "fb5b47ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>dep</th>\n",
       "      <th>tag</th>\n",
       "      <th>children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>please</td>\n",
       "      <td>please</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>intj</td>\n",
       "      <td>UH</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remind</td>\n",
       "      <td>remind</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>VB</td>\n",
       "      <td>[please, I, about]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>me</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>dobj</td>\n",
       "      <td>PRP</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>about</td>\n",
       "      <td>about</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td>IN</td>\n",
       "      <td>[meeting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>det</td>\n",
       "      <td>DT</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meeting</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>NN</td>\n",
       "      <td>[the, on]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "      <td>IN</td>\n",
       "      <td>[telegram]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>telegram</td>\n",
       "      <td>telegram</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "      <td>NN</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text     lemma   pos   dep  tag            children\n",
       "0    please    please  INTJ  intj   UH                  []\n",
       "1    remind    remind  VERB  ROOT   VB  [please, I, about]\n",
       "2        me         I  PRON  dobj  PRP                  []\n",
       "3     about     about   ADP  prep   IN           [meeting]\n",
       "4       the       the   DET   det   DT                  []\n",
       "5   meeting   meeting  NOUN  pobj   NN           [the, on]\n",
       "6        on        on   ADP  prep   IN          [telegram]\n",
       "7  telegram  telegram  NOUN  pobj   NN                  []"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#sentence = \"send a reminder for the meeting at 5 pm to everyone now and create a document for meeting notes then send a message to Rajesh sir.\"\n",
    "#sentence = \"send me a reminder on mail regarding the meeting tomorrow\"\n",
    "sentence = \"tell rupesh to psuh the code\"\n",
    "doc = nlp(sentence)\n",
    "graph = {}\n",
    "\n",
    "\n",
    "def generateTable(doc):\n",
    "\n",
    "    token_text = []\n",
    "    token_lemma = []\n",
    "    token_pos = []\n",
    "    token_tag = []\n",
    "    token_dep = []\n",
    "    token_children =[]\n",
    "    times = []\n",
    "    dates = []\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #get root_node \n",
    "    \n",
    "    for token in doc:\n",
    "        token_text.append(token.text)\n",
    "        token_lemma.append(token.lemma_ )\n",
    "        token_pos.append(token.pos_) \n",
    "        token_tag.append(token.tag_)\n",
    "        token_dep.append(token.dep_)\n",
    "        token_children.append([child.lemma_ for child in token.children])\n",
    "        graph[token] = token.children\n",
    "        if token.dep_==\"ROOT\":\n",
    "            root_node = token\n",
    "\n",
    "    df_dict = {\n",
    "        \"text\":token_text,\n",
    "        \"lemma\":token_lemma,\n",
    "        \"pos\":token_pos,\n",
    "        \"dep\":token_dep,\n",
    "        \"tag\":token_tag,\n",
    "        \"children\":token_children\n",
    "    }   \n",
    "\n",
    "\n",
    "#     get date time\n",
    "    for ent in doc.ents:\n",
    "            print(ent,ent.label_)\n",
    "            if ent.label_ in [\"TIME\", \"DATE\"]:\n",
    "                if ent.label_ == \"TIME\":\n",
    "                    times.append(ent.text)\n",
    "                elif ent.label_ == \"DATE\":\n",
    "                    dates.append(ent.text)\n",
    "    print(dates,times)\n",
    "\n",
    "    df = pd.DataFrame(df_dict)\n",
    "\n",
    "    return df,root_node\n",
    "\n",
    "df , root_node = generateTable(doc)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86aea9f",
   "metadata": {},
   "source": [
    "Find the action in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "2143c062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified Sentence : remind me about the meeting on telegram\n",
      "Action : remind\n",
      "Who : ['I']\n",
      "What: the meeting on telegram\n",
      "When: None\n",
      "Where: telegram\n",
      "{'ACTION': 'remind', 'WHO': ['I'], 'WHAT': 'the meeting on telegram', 'WHEN': None, 'WHERE': 'telegram'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "\n",
    "\n",
    "def break_querry(sentence):\n",
    "    \n",
    "    \n",
    "    #return dict\n",
    "    \n",
    "    answer = {}\n",
    "    \n",
    "    #simplify the sentence and retrieve the root, action\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    simple_sentence = \"\"\n",
    "    action = \"\"\n",
    "    start = False\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.tag_ == \"VBP\":\n",
    "            continue\n",
    "\n",
    "        elif token.dep_==\"ROOT\":\n",
    "            root_node = token\n",
    "            start = True\n",
    "            action+=\" \"+token.lemma_\n",
    "\n",
    "            for t in token.children:\n",
    "\n",
    "                if t.pos_==\"NOUN\" and t.dep_!=\"punct\":\n",
    "                    action+=\" \"+t.lemma_\n",
    "                    got_action = True\n",
    "                    break\n",
    "        \n",
    "        if start:\n",
    "            simple_sentence+=\" \"+token.text\n",
    "\n",
    "\n",
    "    sentence = simple_sentence.strip()\n",
    "    action = action.strip()\n",
    "    \n",
    "    print(\"Simplified Sentence :\",sentence)\n",
    "    \n",
    "    \n",
    "    print(\"Action :\",action)\n",
    "    answer[\"ACTION\"]=action\n",
    "    \n",
    "    #find the target\n",
    "    \n",
    "    stack = [root_node]\n",
    "    visited=[root_node]\n",
    "    target_people = []\n",
    "    while stack:\n",
    "        token = stack.pop()\n",
    "        \n",
    "        for i in token.children:\n",
    "            if i.pos_ in [\"PRON\",\"PROPN\"] and i not in visited:\n",
    "                stack.append(i)\n",
    "                visited.append(i)\n",
    "                target_people.append(i.lemma_)\n",
    "                \n",
    "    \n",
    "    print(\"Who :\",target_people)\n",
    "    answer[\"WHO\"]=target_people\n",
    "    \n",
    "    context = sentence\n",
    "    \n",
    "    \n",
    "    #get what\n",
    "    \n",
    "    scheduled_for = \" \".join(target_people)\n",
    "    q1 = \"What should I \"+action +\"for?\"\n",
    "    w1 = qa_model(question = q1, context = context)\n",
    "    print(\"What:\",w1['answer'])\n",
    "    answer[\"WHAT\"]=w1['answer']\n",
    "    \n",
    "    \n",
    "    #get when parsed from date time\n",
    "\n",
    "    dates = []\n",
    "    times = []\n",
    "    for ent in doc.ents:\n",
    "\n",
    "        if ent.label_ in [\"TIME\", \"DATE\"]:\n",
    "            if ent.label_ == \"TIME\":\n",
    "                times.append(ent.text)\n",
    "            elif ent.label_ == \"DATE\":\n",
    "                dates.append(ent.text)\n",
    "\n",
    "    date = \"\".join(dates)\n",
    "    time = \"\".join(times)\n",
    "    input_text = date + \" \" + time\n",
    "    parsed_datetime = dateparser.parse(input_text)\n",
    "    print(\"When:\",parsed_datetime)\n",
    "    answer[\"WHEN\"]=parsed_datetime\n",
    "\n",
    "    \n",
    "    #get where\n",
    "\n",
    "    q4 = \"Where should I \"+action \n",
    "    w4 = qa_model(question = q4, context = context)\n",
    "    if w4[\"score\"]>0.5:\n",
    "        w4 = w4['answer']\n",
    "    else:\n",
    "        w4 = None\n",
    "\n",
    "    print(\"Where:\",w4)\n",
    "    answer[\"WHERE\"]=w4\n",
    "    \n",
    "    return answer\n",
    "    \n",
    "ans = break_querry(sentence)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "cef98595",
   "metadata": {},
   "outputs": [],
   "source": [
    "functionality_space = {\n",
    "    \"add reminder\":\"ADD MESSAGE\",\n",
    "    \"add message\":\"ADD MESSAGE\",\n",
    "    \"tell\":\"ADD MESSAGE\",\n",
    "    \"send message\":\"SEND MESSAGE\",\n",
    "    \"drop message\":\"ADD MESSAGE\",\n",
    "    \"have message\":\"GET MESSAGE\",\n",
    "    \"remind\":\"ADD MESSAGE\",\n",
    "    \"leave message\":\"ADD MESSAGE\",\n",
    "    \"add\":\"ADD\",\n",
    "    \"create list\":\"CREATE LIST\",\n",
    "    \"add event\":\"ADD TO SCHEDULE\",\n",
    "    \"schedule event\":\"ADD TO SCHEDULE\",\n",
    "    \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "bea3cc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADD MESSAGE'"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_functionality(action):\n",
    "    best_ans = (0,0)\n",
    "\n",
    "    for act in functionality_space:\n",
    "        doc1 = nlp(act)\n",
    "        doc2 = nlp(action)\n",
    "        similarity = doc1.similarity(doc2)\n",
    "        # print(data, similarity)\n",
    "        if similarity > best_ans[0]:\n",
    "            best_ans = (similarity,functionality_space[act])\n",
    "\n",
    "    return best_ans\n",
    "\n",
    "function =  find_functionality(action)[1]\n",
    "function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "0240e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_person = [\n",
    "    \"I\",\n",
    "    \"everyone\",\n",
    "    \"mashaal\",\n",
    "    \"priyansh\",\n",
    "    \"anmol\",\n",
    "    \"divyansh\",\n",
    "    \"rupesh\",\n",
    "    \"rajesh\",\n",
    "    \"ashutosh\",\n",
    "    \"naveen\",\n",
    "    \"harshit\",\n",
    "    \"aswin\",\n",
    "    \"herchelle\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "dfa4b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_objects = [\n",
    "    \"messsage\",\n",
    "    \"schedule\",\n",
    "    \"inventory\",\n",
    "    \"funds\",\n",
    "    \"list\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "212948aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_known_names(sentence, known_names):\n",
    "    found_names = []\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    for token in doc:\n",
    "        closest_match = difflib.get_close_matches(token.lemma_, known_names, n=1, cutoff=0.7)\n",
    "        if closest_match:\n",
    "            found_names.append((token.lemma_, closest_match[0]))\n",
    "    return found_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a634c66e",
   "metadata": {},
   "source": [
    "Check for W words\n",
    "Get action\n",
    "find the person WHO\n",
    "find WHAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "454b14b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: send reminder\n",
      "{'score': 0.8457335233688354, 'start': 53, 'end': 63, 'answer': 'Rajesh sir'}\n",
      "[('I', 'I'), ('rupesh', 'rupesh'), ('Rajesh', 'rajesh')]\n",
      "I\n",
      "What: the meeting with Rajesh sir\n",
      "When: 2024-03-15 22:53:14.783255\n",
      "Where: None\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "\n",
    "def get_details(action,sentence,function):\n",
    "    \n",
    "    \n",
    "    #parse the sentence\n",
    "\n",
    "    print('Action:',action)\n",
    "    \n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    #detect objects in the sentece \n",
    "    objects = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"PROPN\",\"PRON\"]:\n",
    "            objects.append(token.lemma_)\n",
    "    \n",
    "    \n",
    "    context = sentence\n",
    "\n",
    "\n",
    "    #get whom for checking known people\n",
    "    \n",
    "    q2 = \"Whom should I \"+action +\" to?\"\n",
    "    w2 = qa_model(question = q2, context = context)\n",
    "    print(w2)\n",
    "    \n",
    "\n",
    "    found_names = find_known_names(sentence, known_person)\n",
    "    \n",
    "    scheduled_for=found_names[0][0] if found_names else \"\"\n",
    "    print(found_names)\n",
    "    print(scheduled_for)\n",
    "\n",
    "    #what\n",
    "    q1 = \"What should I \"+action +\" to \"+scheduled_for\n",
    "    w1 = qa_model(question = q1, context = context)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"What:\",w1['answer'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #get when parsed from date time\n",
    "\n",
    "    dates = []\n",
    "    times = []\n",
    "    for ent in doc.ents:\n",
    "\n",
    "        if ent.label_ in [\"TIME\", \"DATE\"]:\n",
    "            if ent.label_ == \"TIME\":\n",
    "                times.append(ent.text)\n",
    "            elif ent.label_ == \"DATE\":\n",
    "                dates.append(ent.text)\n",
    "\n",
    "    date = \"\".join(dates)\n",
    "    time = \"\".join(times)\n",
    "    input_text = date + \" \" + time\n",
    "    parsed_datetime = dateparser.parse(input_text)\n",
    "    print(\"When:\",parsed_datetime)\n",
    "\n",
    "\n",
    "    #get where\n",
    "\n",
    "    q4 = \"Where should I \"+action +\" \"+scheduled_for+\" to\" + w1['answer']\n",
    "    w4 = qa_model(question = q4, context = context)\n",
    "    if w4[\"score\"]>0.5:\n",
    "        w4 = w4['answer']\n",
    "    else:\n",
    "        w4 = None\n",
    "\n",
    "    print(\"Where:\",w4)\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "get_details(action ,sentence,function )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "448f13a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found names with nearest spelling:\n",
      "Original: Jhon, Closest: John\n",
      "Original: Alica., Closest: Alice\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "known_names = ['John', 'Alice', 'Bob', 'Charlie', 'David', 'Emily']\n",
    "sentence = \"Hello, my name is Jhon and I'm talking to Alica.\"\n",
    "found_names = find_known_names(sentence, known_names)\n",
    "print(\"Found names with nearest spelling:\")\n",
    "for original, closest in found_names:\n",
    "    print(f\"Original: {original}, Closest: {closest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "63b9e5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ashwin', 'aswin'), ('Rajesh', 'rajesh')]\n"
     ]
    }
   ],
   "source": [
    "def find_known_names(sentence, known_names):\n",
    "    found_names = []\n",
    "    for word in sentence.split():\n",
    "        closest_match = difflib.get_close_matches(word, known_names, n=1, cutoff=0.7)\n",
    "        if closest_match:\n",
    "            found_names.append((word, closest_match[0]))\n",
    "    return found_names\n",
    "\n",
    "found_names = find_known_names(sentence, known_person)\n",
    "print(found_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c0047ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.4729948341846466, 'start': 28, 'end': 36, 'answer': 'tomorrow'}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = \"Who should I remind\"\n",
    "qa_model(question = q2, context  = sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "be615093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: send\n",
      "Information:\n",
      "reminder: NN\n",
      "me: PRP\n",
      "meeting: NN\n",
      "tomorrow: NN\n",
      "Rajesh: NNP\n",
      "sir: NNP\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define a function to extract action and information from a sentence\n",
    "def extract_action_and_info(sentence):\n",
    "    # Process the input sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Initialize variables to store action and information\n",
    "    action = None\n",
    "    information = {}\n",
    "    \n",
    "    # Iterate over each token in the processed sentence\n",
    "    for token in doc:\n",
    "        # Check if the token is a verb (indicating an action)\n",
    "        if token.pos_ == \"VERB\":\n",
    "            action = token.lemma_  # Use the lemma of the verb as the action\n",
    "        # Check if the token is a noun or proper noun (indicating information)\n",
    "        elif token.pos_ in [\"NOUN\", \"PROPN\",\"PRON\"]:\n",
    "            information[token.text] = token.tag_  # Store the noun and its part of speech tag\n",
    "    \n",
    "    return action, information\n",
    "\n",
    "# Example usage\n",
    "sentence = \"send a reminder to me about the meeting tomorrow with Rajesh sir\"\n",
    "action, information = extract_action_and_info(sentence)\n",
    "print(\"Action:\", action)\n",
    "print(\"Information:\")\n",
    "for noun, pos_tag in information.items():\n",
    "    print(f\"{noun}: {pos_tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "22c87963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6087073683738708,\n",
       " 'start': 14,\n",
       " 'end': 31,\n",
       " 'answer': 'to talk to Naveen'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "info = \"tell Priyansh to talk to Naveen sir\"\n",
    "\n",
    "qa_model(question = \"what do I tell Priyansh\", context = info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5ab37eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: send\n",
      "Subject: None\n",
      "Object: message\n",
      "Time: None\n",
      "Other Information: {'me': 'PRP', 'a': 'DT', 'at': 'IN', '12': 'CD', 'about': 'IN', 'the': 'DT', 'with': 'IN'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a function to extract action and information from a sentence\n",
    "def extract_action_and_info(sentence):\n",
    "    # Process the input sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Initialize variables to store action, subject, object, time, and other information\n",
    "    action = None\n",
    "    subject = None\n",
    "    obj = None\n",
    "    time = None\n",
    "    other_info = {}\n",
    "    \n",
    "    # Iterate over each token in the processed sentence\n",
    "    for token in doc:\n",
    "        # Check if the token is a verb (indicating an action)\n",
    "        if token.pos_ == \"VERB\":\n",
    "            action = token.lemma_  # Use the lemma of the verb as the action\n",
    "        # Check if the token is a noun or proper noun (indicating subject or object)\n",
    "        elif token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            # Check if the token is a subject (nsubj) or object (dobj)\n",
    "            if token.dep_ == \"nsubj\":\n",
    "                subject = token.text\n",
    "            elif token.dep_ == \"dobj\":\n",
    "                obj = token.text\n",
    "        # Check if the token is a time expression\n",
    "        elif token.ent_type_ == \"DATE\":\n",
    "            time = token.text\n",
    "        # Store other relevant information\n",
    "        else:\n",
    "            other_info[token.text] = token.tag_\n",
    "    \n",
    "    return action, subject, obj, time, other_info\n",
    "\n",
    "# Example usage\n",
    "sentence = \"send me a reminder message at 12pm today about the meeting tomorrow with Rajesh sir \"\n",
    "action, subject, obj, time, other_info = extract_action_and_info(sentence)\n",
    "print(\"Action:\", action)\n",
    "print(\"Subject:\", subject)\n",
    "print(\"Object:\", obj)\n",
    "print(\"Time:\", time)\n",
    "print(\"Other Information:\", other_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e7b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
